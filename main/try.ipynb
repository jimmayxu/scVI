{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zx3/PycharmProjects/scVI\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs_all = None\n",
    "save_path = 'data/'\n",
    "show_plot = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scvi.inference import UnsupervisedTrainer\n",
    "from scvi.models.vae import VAE\n",
    "from scvi.dataset import  RetinaDataset\n",
    "from scvi.inference.posterior import Posterior\n",
    "\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/retina.loom already downloaded\n",
      "Preprocessing dataset\n",
      "Finished preprocessing dataset\n"
     ]
    }
   ],
   "source": [
    "gene_dataset = RetinaDataset(save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_genes = gene_dataset.nb_genes = 13166\n",
    "n_batches = gene_dataset.n_batches = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=50 \n",
    "lr=1e-3\n",
    "use_batches=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(gene_dataset.nb_genes, n_batch= n_batches * use_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "px_r - torch.Size([13166])\n",
      "z_encoder.encoder.fc_layers.Layer 0.0.weight - torch.Size([128, 13166])\n",
      "z_encoder.encoder.fc_layers.Layer 0.0.bias - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.weight - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.bias - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.running_mean - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.running_var - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.num_batches_tracked - torch.Size([])\n",
      "z_encoder.mean_encoder.weight - torch.Size([10, 128])\n",
      "z_encoder.mean_encoder.bias - torch.Size([10])\n",
      "z_encoder.var_encoder.weight - torch.Size([10, 128])\n",
      "z_encoder.var_encoder.bias - torch.Size([10])\n",
      "l_encoder.encoder.fc_layers.Layer 0.0.weight - torch.Size([128, 13166])\n",
      "l_encoder.encoder.fc_layers.Layer 0.0.bias - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.weight - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.bias - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.running_mean - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.running_var - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.num_batches_tracked - torch.Size([])\n",
      "l_encoder.mean_encoder.weight - torch.Size([1, 128])\n",
      "l_encoder.mean_encoder.bias - torch.Size([1])\n",
      "l_encoder.var_encoder.weight - torch.Size([1, 128])\n",
      "l_encoder.var_encoder.bias - torch.Size([1])\n",
      "decoder.px_decoder.fc_layers.Layer 0.0.weight - torch.Size([128, 12])\n",
      "decoder.px_decoder.fc_layers.Layer 0.0.bias - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.weight - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.bias - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.running_mean - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.running_var - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.num_batches_tracked - torch.Size([])\n",
      "decoder.px_scale_decoder.0.weight - torch.Size([13166, 128])\n",
      "decoder.px_scale_decoder.0.bias - torch.Size([13166])\n",
      "decoder.px_r_decoder.weight - torch.Size([13166, 128])\n",
      "decoder.px_r_decoder.bias - torch.Size([13166])\n",
      "decoder.px_dropout_decoder.weight - torch.Size([13166, 128])\n",
      "decoder.px_dropout_decoder.bias - torch.Size([13166])\n"
     ]
    }
   ],
   "source": [
    "load_dict = torch.load(\"saved_model/ave_dict.pt\",map_location='cpu')\n",
    "_ = [print(key,\"-\", values.shape) for (key,values) in zip(load_dict.keys(),load_dict.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE:\n\tsize mismatch for decoder.px_decoder.fc_layers.Layer 0.0.weight: copying a param with shape torch.Size([128, 12]) from checkpoint, the shape in current model is torch.Size([128, 10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fa7fb94159c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE:\n\tsize mismatch for decoder.px_decoder.fc_layers.Layer 0.0.weight: copying a param with shape torch.Size([128, 12]) from checkpoint, the shape in current model is torch.Size([128, 10])."
     ]
    }
   ],
   "source": [
    "vae.load_state_dict(load_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "px_r - torch.Size([13166])\n",
      "z_encoder.encoder.fc_layers.Layer 0.0.weight - torch.Size([128, 13166])\n",
      "z_encoder.encoder.fc_layers.Layer 0.0.bias - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.weight - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.bias - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.running_mean - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.running_var - torch.Size([128])\n",
      "z_encoder.encoder.fc_layers.Layer 0.1.num_batches_tracked - torch.Size([])\n",
      "z_encoder.mean_encoder.weight - torch.Size([10, 128])\n",
      "z_encoder.mean_encoder.bias - torch.Size([10])\n",
      "z_encoder.var_encoder.weight - torch.Size([10, 128])\n",
      "z_encoder.var_encoder.bias - torch.Size([10])\n",
      "l_encoder.encoder.fc_layers.Layer 0.0.weight - torch.Size([128, 13166])\n",
      "l_encoder.encoder.fc_layers.Layer 0.0.bias - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.weight - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.bias - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.running_mean - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.running_var - torch.Size([128])\n",
      "l_encoder.encoder.fc_layers.Layer 0.1.num_batches_tracked - torch.Size([])\n",
      "l_encoder.mean_encoder.weight - torch.Size([1, 128])\n",
      "l_encoder.mean_encoder.bias - torch.Size([1])\n",
      "l_encoder.var_encoder.weight - torch.Size([1, 128])\n",
      "l_encoder.var_encoder.bias - torch.Size([1])\n",
      "decoder.px_decoder.fc_layers.Layer 0.0.weight - torch.Size([128, 10])\n",
      "decoder.px_decoder.fc_layers.Layer 0.0.bias - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.weight - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.bias - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.running_mean - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.running_var - torch.Size([128])\n",
      "decoder.px_decoder.fc_layers.Layer 0.1.num_batches_tracked - torch.Size([])\n",
      "decoder.px_scale_decoder.0.weight - torch.Size([13166, 128])\n",
      "decoder.px_scale_decoder.0.bias - torch.Size([13166])\n",
      "decoder.px_r_decoder.weight - torch.Size([13166, 128])\n",
      "decoder.px_r_decoder.bias - torch.Size([13166])\n",
      "decoder.px_dropout_decoder.weight - torch.Size([13166, 128])\n",
      "decoder.px_dropout_decoder.bias - torch.Size([13166])\n"
     ]
    }
   ],
   "source": [
    "dict = vae.state_dict()\n",
    "_ = [print(key,\"-\", values.shape) for (key,values) in zip(dict.keys(),dict.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"batch_size\": 128,\n",
    "    \"pin_memory\": True\n",
    "}\n",
    "full = Posterior(vae, gene_dataset, indices=np.arange(len(gene_dataset)), use_cuda=False,\n",
    "                          data_loader_kwargs=kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post = Posterior(vae, gene_dataset, indices=np.arange(len(gene_dataset)), use_cuda=False,\n",
    "                          data_loader_kwargs=kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Scores:\n",
      "Silhouette: 0.4064\n",
      "NMI: 0.7976\n",
      "ARI: 0.5612\n",
      "UCA: 0.6158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/team205/zx3/.venv/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/nfs/team205/zx3/.venv/lib/python3.7/site-packages/sklearn/utils/linear_assignment_.py:127: DeprecationWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "_, NMI,_,_ = full.clustering_scores(prediction_algorithm = \"gmm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.show_t_sne()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
